{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the required libraries and Models"
      ],
      "metadata": {
        "id": "M-eULD0vjFQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ISn-WT9i0gE",
        "outputId": "e47e4a77-9454-4da2-f275-d3d9d9a4edff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "build_text and safe_get functions imported from text_helpers.py\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, normalizers, processors\n",
        "import json\n",
        "from typing import List, Tuple, Dict, Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.utils import class_weight\n",
        "import joblib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F # Added this import\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/AAI-590/')\n",
        "from text_helpers import build_text, safe_get\n",
        "\n",
        "print(\"build_text and safe_get functions imported from text_helpers.py\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer when needed\n",
        "tokenizer = Tokenizer.from_file(\"/content/drive/MyDrive/AAI-590/tokenizer.json\")"
      ],
      "metadata": {
        "id": "WjWuMqQqwOdj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attr_model_path = \"/content/drive/MyDrive/AAI-590/models_attributes.pkl\"\n",
        "sentiment_tfidf_model_path = \"/content/drive/MyDrive/AAI-590/models_sentiment.pkl\"\n",
        "sentiment_model_path = \"/content/drive/MyDrive/AAI-590/sentiment_analysis_model.pth\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size= 12000\n",
        "cfg = {\n",
        "    \"vocab_size\":vocab_size,\n",
        "    \"emb_dim\":128,\n",
        "    \"hidden_dim\": 512,\n",
        "    \"num_layers\":5,\n",
        "    \"bidirectional\": True,\n",
        "    \"dropout\": 0.15,\n",
        "    \"seq_len\":256,\n",
        "}\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "class SentimentAnalysisModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "      super(SentimentAnalysisModel, self).__init__()\n",
        "      self.embedding = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "      self.lstm = nn.LSTM(input_size=cfg['emb_dim'],\n",
        "                          hidden_size=cfg['hidden_dim'],\n",
        "                          num_layers=cfg['num_layers'],\n",
        "                          batch_first=True,\n",
        "                          bidirectional=cfg['bidirectional'],\n",
        "                          dropout=cfg['dropout'])\n",
        "      self.conv1 = nn.Conv1d(in_channels=cfg['hidden_dim'] * 2,\n",
        "                            out_channels=128,\n",
        "                            kernel_size=3)\n",
        "      self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "      #input: (batch_size,128,128)\n",
        "\n",
        "      # Correct calculation for in_features for fc1:\n",
        "      # (seq_len - kernel_size + 1) // pool_kernel_size = (256 - 3 + 1) // 2 = 254 // 2 = 127\n",
        "      # 128 * 127 = 16256\n",
        "      self.fc1 = nn.Linear(128 * 127, 64)\n",
        "      #output: (batch_size , 64)\n",
        "\n",
        "      # Final layer for 3 sentiment classes (negative, neutral, positive)\n",
        "      self.fc2 = nn.Linear(64, 3)\n",
        "      #output: (batch_size, 3) - raw logits\n",
        "\n",
        "      self.dropout = nn.Dropout(cfg['dropout'])\n",
        "      # No sigmoid here, CrossEntropyLoss expects raw logits\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.embedding(x)\n",
        "      lstm_out, _ = self.lstm(x)\n",
        "      lstm_out = lstm_out.permute(0, 2, 1)\n",
        "      conv_out = self.conv1(lstm_out)\n",
        "      pooled_out = self.pool(conv_out)\n",
        "\n",
        "      # flatten\n",
        "      flattened = pooled_out.view(pooled_out.size(0), -1)\n",
        "\n",
        "      x = self.dropout(torch.relu(self.fc1(flattened)))\n",
        "      # Output raw logits for CrossEntropyLoss\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "loaded_attr_clf = None\n",
        "loaded_mlb = None\n",
        "loaded_sentiment_clf = None\n",
        "loaded_sentiment_tdfif_clf = None\n",
        "\n",
        "try:\n",
        "    loaded_attr_model_data = joblib.load(attr_model_path)\n",
        "    loaded_attr_clf = loaded_attr_model_data[\"pipeline\"]\n",
        "    loaded_mlb = loaded_attr_model_data[\"mlb\"]\n",
        "    print(\"Attribute model and MultiLabelBinarizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading attribute model: {e}\")\n",
        "\n",
        "try:\n",
        "    loaded_sentiment_tdfif_clf = joblib.load(sentiment_tfidf_model_path)\n",
        "    print(\"Sentiment TF-IDF model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sentiment TF-IDF model: {e}\")\n",
        "\n",
        "try:\n",
        "    loaded_sentiment_clf = SentimentAnalysisModel(cfg)\n",
        "    loaded_sentiment_clf.load_state_dict(torch.load(sentiment_model_path, map_location=device))\n",
        "    loaded_sentiment_clf.eval()\n",
        "    print(\"Sentiment model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sentiment model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aGIm0_4nTdn",
        "outputId": "89947d6f-d5b9-435c-8b59-0c17619c7219"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attribute model and MultiLabelBinarizer loaded successfully.\n",
            "Sentiment TF-IDF model loaded successfully.\n",
            "Sentiment model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, text, tokenizer, device, cfg):\n",
        "    model.eval()\n",
        "    cleaned_text = clean_text(text)\n",
        "    encoded = tokenizer.encode(cleaned_text)\n",
        "    input_ids = torch.tensor([encoded.ids]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "    # Get the predicted class index and its probability\n",
        "    predicted_probability, predicted_index = torch.max(probabilities, dim=1)\n",
        "\n",
        "    # Map numerical index back to sentiment label\n",
        "    label_map_reverse = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "    sentiment = label_map_reverse[predicted_index.item()]\n",
        "\n",
        "    return sentiment, predicted_probability.item()"
      ],
      "metadata": {
        "id": "NXYOLND8uQwN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_analyze = \"design looked good but the cloth is bad. Don't buy it\"\n",
        "sentiment, probability = inference(loaded_sentiment_clf, text_to_analyze, tokenizer, device, cfg)\n",
        "print(f\"Sentiment: {sentiment}\")\n",
        "print(f\"Probability: {probability:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc6bec2z5Tpd",
        "outputId": "dfd7be2b-5bec-48ce-a48b-70ce05b60618"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: negative\n",
            "Probability: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_uploaded_csv(file):\n",
        "    if loaded_attr_clf is None or loaded_mlb is None or loaded_sentiment_clf is None:\n",
        "        return pd.DataFrame({'Error': ['ML models not loaded. Please ensure the model files exist and are accessible.']}), None, None, None\n",
        "\n",
        "    if file is None:\n",
        "        return pd.DataFrame({'Message': ['Please upload a CSV file.']}), None, None, None\n",
        "\n",
        "    try:\n",
        "        df_uploaded = pd.read_csv(file.name)\n",
        "        # Drop rows from df_uploaded where Title and ReviewText both are empty or Null\n",
        "        df_uploaded = df_uploaded.dropna(subset=['Title', 'ReviewText'], how='all').reset_index(drop=True)\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({'Error': [f'Failed to read CSV: {e}']}), None, None, None\n",
        "\n",
        "    # Ensure 'Title' and 'ReviewText' columns exist, fill missing with empty string\n",
        "    if 'Title' not in df_uploaded.columns:\n",
        "        df_uploaded['Title'] = ''\n",
        "    if 'ReviewText' not in df_uploaded.columns:\n",
        "        df_uploaded['ReviewText'] = ''\n",
        "\n",
        "    # Apply build_text and safe_get for combined text\n",
        "    df_uploaded[\"__text__\"] = df_uploaded.apply(lambda row: build_text(\n",
        "        safe_get(row, \"Title\"),\n",
        "        safe_get(row, \"ReviewText\")\n",
        "    ), axis=1)\n",
        "\n",
        "    # --- ML Attribute Extraction ---\n",
        "    try:\n",
        "        # Ensure TF-IDF transform is applied before decision_function if using pipeline's decision_function\n",
        "        attr_proba = loaded_attr_clf.predict_proba(df_uploaded[\"__text__\"].values)\n",
        "    except Exception:\n",
        "        # Fallback for decision_function + per-class sigmoid for consistency with training\n",
        "        # Need to transform text using the pipeline's TFIDF vectorizer first\n",
        "        scores = loaded_attr_clf.named_steps['tfidf'].transform(df_uploaded[\"__text__\"].values)\n",
        "        attr_proba = expit(scores)\n",
        "\n",
        "    attr_pred_bin = (attr_proba >= 0.5).astype(int)\n",
        "    ml_attr_tags = loaded_mlb.inverse_transform(attr_pred_bin)\n",
        "    df_uploaded['Extracted_Attributes'] = [\"; \".join(tags) for tags in ml_attr_tags]\n",
        "\n",
        "    # --- ML Sentiment Analysis ---\n",
        "    all_sentiments = []\n",
        "    all_probabilities = []\n",
        "    # Loop through each text entry and call the inference function\n",
        "    for text_item in df_uploaded[\"__text__\"].values:\n",
        "        sentiment_label, sentiment_prob = inference(loaded_sentiment_clf, text_item, tokenizer, device, cfg)\n",
        "        all_sentiments.append(sentiment_label)\n",
        "        all_probabilities.append(sentiment_prob)\n",
        "\n",
        "    df_uploaded[\"ML_Sentiment_Label\"] = all_sentiments\n",
        "    df_uploaded[\"ML_Sentiment_Score\"] = all_probabilities\n",
        "\n",
        "    # --- TFIDF Sentiment Analysis ---\n",
        "    try:\n",
        "        proba_all_senti = loaded_sentiment_tdfif_clf.predict_proba(df_uploaded[\"__text__\"].values)\n",
        "        senti_classes = loaded_sentiment_tdfif_clf.named_steps[\"logreg\"].classes_\n",
        "        prob_df_senti = pd.DataFrame(proba_all_senti, columns=[f\"ML_Sentiment_Prob_{c}\" for c in senti_classes])\n",
        "    except Exception:\n",
        "        scores_senti = loaded_sentiment_tdfif_clf.decision_function(df_uploaded[\"__text__\"].values)\n",
        "        if scores_senti.ndim == 1:\n",
        "            scores_senti = np.vstack([scores_senti, -scores_senti]).T\n",
        "            senti_classes = np.array([\"Positive\", \"Negative\"])\n",
        "        else:\n",
        "            senti_classes = loaded_sentiment_tdfif_clf.named_steps[\"logreg\"].classes_\n",
        "        proba_all_senti = softmax(scores_senti, axis=1)\n",
        "        prob_df_senti = pd.DataFrame(proba_all_senti, columns=[f\"ML_Sentiment_Prob_{c}\" for c in senti_classes])\n",
        "\n",
        "    senti_pred = loaded_sentiment_tdfif_clf.predict(df_uploaded[\"__text__\"].values)\n",
        "    df_uploaded[\"TFIDF_Sentiment_Label\"] = senti_pred\n",
        "\n",
        "    pos_col = [c for c in prob_df_senti.columns if c.endswith(\"positive\")]\n",
        "    neg_col = [c for c in prob_df_senti.columns if c.endswith(\"negative\")]\n",
        "    pos_prob = prob_df_senti[pos_col[0]] if pos_col else 0.0\n",
        "    neg_prob = prob_df_senti[neg_col[0]] if neg_col else 0.0\n",
        "    df_uploaded[\"TFIDF_Sentiment_Score\"] = (pos_prob - neg_prob).fillna(0.0)\n",
        "\n",
        "    # Select relevant columns for display\n",
        "    output_df = df_uploaded[['Title', 'ReviewText', 'Extracted_Attributes', 'ML_Sentiment_Label', 'ML_Sentiment_Score', 'TFIDF_Sentiment_Label']]\n",
        "\n",
        "    # --- Generate Bar Chart for Sentiment Distribution ---\n",
        "    sentiment_counts = df_uploaded['ML_Sentiment_Label'].value_counts().sort_index()\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')\n",
        "    plt.title('Distribution of ML Sentiment Labels')\n",
        "    plt.xlabel('Sentiment Label')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    sentiment_plot_bytes = io.BytesIO()\n",
        "    plt.savefig(sentiment_plot_bytes, format='png')\n",
        "    plt.close() # Close the plot to free memory\n",
        "    sentiment_plot_bytes.seek(0)\n",
        "    sentiment_plot_pil = Image.open(sentiment_plot_bytes) # Convert to PIL Image\n",
        "\n",
        "    # --- Generate Word Cloud for Extracted Attributes ---\n",
        "    # Filter out empty strings and then join\n",
        "    all_attributes = \" \".join([attr for attr_list in df_uploaded['Extracted_Attributes'].tolist() for attr in attr_list.split('; ') if attr.strip()])\n",
        "\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_attributes)\n",
        "    wordcloud_bytes = io.BytesIO()\n",
        "    wordcloud.to_image().save(wordcloud_bytes, format='png')\n",
        "    wordcloud_bytes.seek(0)\n",
        "    wordcloud_pil = Image.open(wordcloud_bytes) # Convert to PIL Image\n",
        "\n",
        "    # --- Save output_df to a temporary CSV for download ---\n",
        "    temp_csv_path = \"processed_reviews.csv\"\n",
        "    output_df.to_csv(temp_csv_path, index=False)\n",
        "\n",
        "    #--- Create a list of top 10 most used attributes\n",
        "    all_attributes_list = [attr.strip() for attr_list in df_uploaded['Extracted_Attributes'].tolist() for attr in attr_list.split('; ') if attr.strip()]\n",
        "    attribute_counts = pd.Series(all_attributes_list).value_counts().head(10)\n",
        "    top_10_attributes = attribute_counts.index.tolist()\n",
        "\n",
        "\n",
        "    return output_df, sentiment_plot_pil, wordcloud_pil, temp_csv_path, top_10_attributes"
      ],
      "metadata": {
        "id": "kBkj1RA1lsBy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom CSS and head content\n",
        "custom_css = \"\"\"\n",
        "body { font-family: 'Arial', sans-serif; background-color: #f0f2f5; }\n",
        ".gradio-container { max-width: 1200px; margin: auto; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 8px; background-color: #bbc8f2; }\n",
        ".gradio-input label, .gradio-output label { font-weight: bold; color: #333; }\n",
        ".gradio-title { color: #7faaeb; text-align: center; margin-bottom: 20px; }\n",
        ".gradio-description { text-align: center; color: #555; margin-bottom: 30px; }\n",
        ".gr-button { color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; }\n",
        ".gr-button:hover { background-color: #28a745; }\n",
        "/* CSS for text wrapping in DataFrame cells */\n",
        ".gradio-output--grid table td { white-space: normal !important; }\n",
        "\n",
        "/* Custom styles for process button states using an ID for higher specificity */\n",
        "#process_csv_button.no-file-button {\n",
        "    background-color: #28a745 !important; /* Grey when no file is uploaded */\n",
        "}\n",
        "#process_csv_button.enabled-button {\n",
        "    background-color: #28a745 !important; /* Green when active */\n",
        "}\n",
        "#process_csv_button.enabled-button:hover {\n",
        "    background-color: #28a745 !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "custom_head = \"<title>AAI590-Capstone Project</title>\"\n",
        "\n",
        "# Create the Gradio Blocks interface\n",
        "with gr.Blocks(css=custom_css, head=custom_head, title=\"AAI590:ML-based Attribute and Sentiment Extraction and Visualization\") as demo:\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=20):\n",
        "            gr.Markdown(\"# AAI590: ML-based Attribute and Sentiment Extraction and Visualization\")\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Image(\"/content/drive/MyDrive/AAI-590/USD_Logo.png\", width=50, elem_id=\"logo\", show_download_button=False)\n",
        "\n",
        "    gr.Markdown(\"### By Aditya, Deepak, Rajesh\")\n",
        "    gr.Markdown(\"Upload a CSV file containing 'Title' and 'ReviewText' columns to extract product attributes and sentiment, and visualize their distribution and common attributes.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"Upload CSV File\")\n",
        "        # Initially grey but interactive\n",
        "        process_button = gr.Button(\n",
        "            \"Process CSV\",\n",
        "            elem_id=\"process_csv_button\", # Added unique ID here\n",
        "            elem_classes=[\"no-file-button\"],\n",
        "            interactive=True\n",
        "        )\n",
        "\n",
        "    with gr.Row(): # New row for plots\n",
        "        sentiment_plot_component = gr.Image(label=\"ML Sentiment Distribution\", type=\"pil\")\n",
        "        wordcloud_plot_component = gr.Image(label=\"Extracted Attributes Word Cloud\", type=\"pil\")\n",
        "\n",
        "    with gr.Row(): # New row for DataFrame and download with width control\n",
        "        with gr.Column(): # 3/4 width for DataFrame\n",
        "            output_df_component = gr.DataFrame(label=\"Processed Reviews with Extracted Attributes and Sentiment\")\n",
        "    with gr.Row(): # 1/4 width for download button\n",
        "            download_csv_component = gr.File(label=\"Download Processed Data (.csv)\")\n",
        "    with gr.Row():\n",
        "        top_10_attributes_component = gr.Textbox(label=\"Top 10 Most Used Attributes\")\n",
        "\n",
        "    # Function to update button state and style based on file input\n",
        "    def update_button_state(file):\n",
        "        if file is not None:\n",
        "            return gr.Button.update(interactive=True, elem_classes=[\"enabled-button\"])\n",
        "        else:\n",
        "            return gr.Button.update(interactive=True, elem_classes=[\"no-file-button\"])\n",
        "\n",
        "    # Attach the update function to the file input's change event\n",
        "    file_input.change(\n",
        "        fn=update_button_state,\n",
        "        inputs=[file_input],\n",
        "        outputs=[process_button],\n",
        "        queue=False # Important for responsiveness\n",
        "    )\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_uploaded_csv,\n",
        "        inputs=[file_input],\n",
        "        outputs=[output_df_component, sentiment_plot_component, wordcloud_plot_component, download_csv_component, top_10_attributes_component]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "Irg3LHujz1hm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "38f6e365-0e28-4257-f068-7894ba5ad1a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0d4152435936b8718f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0d4152435936b8718f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfyl2SMM_mez"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}