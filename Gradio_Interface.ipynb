{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the required libraries and Models"
      ],
      "metadata": {
        "id": "M-eULD0vjFQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ISn-WT9i0gE",
        "outputId": "52f09334-0ed5-420e-a548-b919f318dfef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "build_text and safe_get functions imported from text_helpers.py\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import json\n",
        "from typing import List, Tuple, Dict, Set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.utils import class_weight\n",
        "import joblib\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/AAI-590/')\n",
        "from text_helpers import build_text, safe_get\n",
        "\n",
        "print(\"build_text and safe_get functions imported from text_helpers.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attr_model_path = \"/content/drive/MyDrive/AAI-590/models_attributes.pkl\"\n",
        "sentiment_model_path = \"/content/drive/MyDrive/AAI-590/models_sentiment.pkl\"\n",
        "\n",
        "loaded_attr_clf = None\n",
        "loaded_mlb = None\n",
        "loaded_sentiment_clf = None\n",
        "\n",
        "try:\n",
        "    loaded_attr_model_data = joblib.load(attr_model_path)\n",
        "    loaded_attr_clf = loaded_attr_model_data[\"pipeline\"]\n",
        "    loaded_mlb = loaded_attr_model_data[\"mlb\"]\n",
        "    print(\"Attribute model and MultiLabelBinarizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading attribute model: {e}\")\n",
        "\n",
        "try:\n",
        "    loaded_sentiment_clf = joblib.load(sentiment_model_path)\n",
        "    print(\"Sentiment model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sentiment model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aGIm0_4nTdn",
        "outputId": "9381612f-8682-44de-9d71-171f6d3a46f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attribute model and MultiLabelBinarizer loaded successfully.\n",
            "Sentiment model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def process_uploaded_csv(file):\n",
        "    if loaded_attr_clf is None or loaded_mlb is None or loaded_sentiment_clf is None:\n",
        "        return pd.DataFrame({'Error': ['ML models not loaded. Please ensure the model files exist and are accessible.']}), None, None\n",
        "\n",
        "    if file is None:\n",
        "        return pd.DataFrame({'Message': ['Please upload a CSV file.']}), None, None\n",
        "\n",
        "    try:\n",
        "        df_uploaded = pd.read_csv(file.name)\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({'Error': [f'Failed to read CSV: {e}']}), None, None\n",
        "\n",
        "    # Ensure 'Title' and 'ReviewText' columns exist, fill missing with empty string\n",
        "    if 'Title' not in df_uploaded.columns:\n",
        "        df_uploaded['Title'] = ''\n",
        "    if 'ReviewText' not in df_uploaded.columns:\n",
        "        df_uploaded['ReviewText'] = ''\n",
        "\n",
        "    # Apply build_text and safe_get for combined text\n",
        "    df_uploaded[\"__text__\"] = df_uploaded.apply(lambda row: build_text(\n",
        "        safe_get(row, \"Title\"),\n",
        "        safe_get(row, \"ReviewText\")\n",
        "    ), axis=1)\n",
        "\n",
        "    # --- ML Attribute Extraction ---\n",
        "    try:\n",
        "        # Ensure TF-IDF transform is applied before decision_function if using pipeline's decision_function\n",
        "        attr_proba = loaded_attr_clf.predict_proba(df_uploaded[\"__text__\"].values)\n",
        "    except Exception:\n",
        "        # Fallback for decision_function + per-class sigmoid for consistency with training\n",
        "        # Need to transform text using the pipeline's TFIDF vectorizer first\n",
        "        scores = loaded_attr_clf.decision_function(loaded_attr_clf.named_steps['tfidf'].transform(df_uploaded[\"__text__\"].values))\n",
        "        attr_proba = expit(scores)\n",
        "\n",
        "    attr_pred_bin = (attr_proba >= 0.5).astype(int)\n",
        "    ml_attr_tags = loaded_mlb.inverse_transform(attr_pred_bin)\n",
        "    df_uploaded['Extracted_Attributes'] = [\"; \".join(tags) for tags in ml_attr_tags]\n",
        "\n",
        "    # --- ML Sentiment Analysis ---\n",
        "    try:\n",
        "        proba_all_senti = loaded_sentiment_clf.predict_proba(df_uploaded[\"__text__\"].values)\n",
        "        senti_classes = loaded_sentiment_clf.named_steps[\"logreg\"].classes_\n",
        "        prob_df_senti = pd.DataFrame(proba_all_senti, columns=[f\"ML_Sentiment_Prob_{c}\" for c in senti_classes])\n",
        "    except Exception:\n",
        "        scores_senti = loaded_sentiment_clf.decision_function(df_uploaded[\"__text__\"].values)\n",
        "        if scores_senti.ndim == 1:\n",
        "            scores_senti = np.vstack([scores_senti, -scores_senti]).T\n",
        "            senti_classes = np.array([\"Positive\", \"Negative\"])\n",
        "        else:\n",
        "            senti_classes = loaded_sentiment_clf.named_steps[\"logreg\"].classes_\n",
        "        proba_all_senti = softmax(scores_senti, axis=1)\n",
        "        prob_df_senti = pd.DataFrame(proba_all_senti, columns=[f\"ML_Sentiment_Prob_{c}\" for c in senti_classes])\n",
        "\n",
        "    senti_pred = loaded_sentiment_clf.predict(df_uploaded[\"__text__\"].values)\n",
        "    df_uploaded[\"ML_Sentiment_Label\"] = senti_pred\n",
        "\n",
        "    pos_col = [c for c in prob_df_senti.columns if c.endswith(\"positive\")]\n",
        "    neg_col = [c for c in prob_df_senti.columns if c.endswith(\"negative\")]\n",
        "    pos_prob = prob_df_senti[pos_col[0]] if pos_col else 0.0\n",
        "    neg_prob = prob_df_senti[neg_col[0]] if neg_col else 0.0\n",
        "    df_uploaded[\"ML_Sentiment_Score\"] = (pos_prob - neg_prob).fillna(0.0)\n",
        "\n",
        "    # Select relevant columns for display\n",
        "    output_df = df_uploaded[['Title', 'ReviewText', 'Extracted_Attributes', 'ML_Sentiment_Label', 'ML_Sentiment_Score']]\n",
        "\n",
        "    # --- Generate Bar Chart for Sentiment Distribution ---\n",
        "    sentiment_counts = df_uploaded['ML_Sentiment_Label'].value_counts().sort_index()\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')\n",
        "    plt.title('Distribution of ML Sentiment Labels')\n",
        "    plt.xlabel('Sentiment Label')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    sentiment_plot_bytes = io.BytesIO()\n",
        "    plt.savefig(sentiment_plot_bytes, format='png')\n",
        "    plt.close() # Close the plot to free memory\n",
        "    sentiment_plot_bytes.seek(0)\n",
        "    sentiment_plot_pil = Image.open(sentiment_plot_bytes) # Convert to PIL Image\n",
        "\n",
        "    # --- Generate Word Cloud for Extracted Attributes ---\n",
        "    # Filter out empty strings and then join\n",
        "    all_attributes = \" \".join([attr for attr_list in df_uploaded['Extracted_Attributes'].tolist() for attr in attr_list.split('; ') if attr.strip()])\n",
        "\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_attributes)\n",
        "    wordcloud_bytes = io.BytesIO()\n",
        "    wordcloud.to_image().save(wordcloud_bytes, format='png')\n",
        "    wordcloud_bytes.seek(0)\n",
        "    wordcloud_pil = Image.open(wordcloud_bytes) # Convert to PIL Image\n",
        "\n",
        "    # --- Save output_df to a temporary CSV for download ---\n",
        "    temp_csv_path = \"processed_reviews.csv\"\n",
        "    output_df.to_csv(temp_csv_path, index=False)\n",
        "\n",
        "    return output_df, sentiment_plot_pil, wordcloud_pil, temp_csv_path\n",
        "\n",
        "# Define custom CSS and head content\n",
        "custom_css = \"\"\"\n",
        "body { font-family: 'Arial', sans-serif; background-color: #f0f2f5; }\n",
        ".gradio-container { max-width: 1200px; margin: auto; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); border-radius: 8px; background-color: #bbc8f2; }\n",
        ".gradio-input label, .gradio-output label { font-weight: bold; color: #333; }\n",
        ".gradio-title { color: #7faaeb; text-align: center; margin-bottom: 20px; }\n",
        ".gradio-description { text-align: center; color: #555; margin-bottom: 30px; }\n",
        ".gr-button { background-color: #007bff; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; }\n",
        ".gr-button:hover { background-color: #0056b3; }\n",
        "/* CSS for text wrapping in DataFrame cells */\n",
        ".gradio-output--grid table td { white-space: normal !important; }\n",
        "\"\"\"\n",
        "\n",
        "custom_head = \"<title>AAI590-Capstone Project</title>\"\n",
        "\n",
        "# Create the Gradio Blocks interface\n",
        "with gr.Blocks(css=custom_css, head=custom_head, title=\"AAI590:ML-based Attribute and Sentiment Extraction and Visualization\") as demo:\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Image(\"USD_Logo.png\", width=50, elem_id=\"logo\", show_download_button=False)\n",
        "        with gr.Column(scale=20):\n",
        "            gr.Markdown(\"# AAI590: ML-based Attribute and Sentiment Extraction and Visualization\")\n",
        "\n",
        "    gr.Markdown(\"### By Aditya, Deepak, Rajesh\")\n",
        "    gr.Markdown(\"Upload a CSV file containing 'Title' and 'ReviewText' columns to extract product attributes and sentiment, and visualize their distribution and common attributes.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"Upload CSV File\")\n",
        "        process_button = gr.Button(\"Process CSV\")\n",
        "\n",
        "    with gr.Row(): # New row for plots\n",
        "        sentiment_plot_component = gr.Image(label=\"ML Sentiment Distribution\", type=\"pil\")\n",
        "        wordcloud_plot_component = gr.Image(label=\"Extracted Attributes Word Cloud\", type=\"pil\")\n",
        "\n",
        "    with gr.Row(): # New row for DataFrame and download with width control\n",
        "        with gr.Column(): # 3/4 width for DataFrame\n",
        "            output_df_component = gr.DataFrame(label=\"Processed Reviews with Extracted Attributes and Sentiment\")\n",
        "    with gr.Row(): # 1/4 width for download button\n",
        "            download_csv_component = gr.File(label=\"Download Processed Data (.csv)\")\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_uploaded_csv,\n",
        "        inputs=[file_input],\n",
        "        outputs=[output_df_component, sentiment_plot_component, wordcloud_plot_component, download_csv_component]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "kBkj1RA1lsBy",
        "outputId": "cd1054e3-98ed-4a14-eb57-1ec9afd6fbe5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bc7a9b3bdde4f92c4b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bc7a9b3bdde4f92c4b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ciiLom0dl1iZ"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}